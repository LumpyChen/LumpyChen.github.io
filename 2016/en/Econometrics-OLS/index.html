<!DOCTYPE html><html lang="en,zh-cn,default"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> Econometrics - OLS · LANTAU</title><meta name="description" content="My econometrics notes, OLS part."><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="short icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="http://lumpychen.github.io/atom.xml" title="LANTAU"></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/favicon.png"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link active">ARCHIVE</a></li><li class="nav-list-item"><a href="https://www.zhihu.com/people/irvingchen" target="_blank" class="nav-list-link">ZHIHU</a></li><li class="nav-list-item"><a href="www.lumpychen.com/cv" target="_self" class="nav-list-link">RESUME</a></li><li class="nav-list-item"><a href="https://github.com/lumpychen" target="_blank" class="nav-list-link">GITHUB</a></li></ul></header><section class="license"><a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png"></a><div class="license-text">Every post in this blog is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>. You can check the source code at <a href="https://github.com/LumpyChen/LumpyChen.github.io">my github</a>.</div></section><section class="container"><div class="post"><article class="post-block"><h1 class="post-title">Econometrics - OLS</h1><div class="post-info">Oct 6, 2016</div><div class="post-tags"><a href="/tags/course/" class="post-tag"> course</a></div><div class="post-content"><link rel="stylesheet" href="/owl.css"><p>This article contains notes of chapter 2 – Ordinary Least Squares.</p>
<p>In the first place, we build an agreement that the purpose of regression analysis is to take a purely theoretical equation:</p>
<p>$$ Y_i=\beta_0+\beta_1X_i+\varepsilon_i $$</p>
<p>and use a set of data to create an estimated equation:</p>
<p>$$ \hat{Y_i}=\hat{\beta_0}+\hat{\beta_1}X_i $$</p>
<p>\(\hat{Y_i}\) is the same as \(E(Y|X)\)</p>
<p>Then, the emphasis of this chapter is on understanding what OLS attempts to do and how it goes about doing it.</p>
<a id="more"></a>
<h2 id="OLS"><a href="#OLS" class="headerlink" title="OLS"></a>OLS</h2><p>Ordinary Least Squares calcutes the \(\hat{\beta}\) so as to minimize the sum of the squared residuals:</p>
<p>$$ OLS_{min} \sum_{i=1}^N e_i^2 (i=1,2,…,N) $$</p>
<p>We can claim that Ordinary Least Square minimizes the value of \(\sum(Y_i-\hat{Y_i})\)</p>
<p>Why we use OLS?</p>
<ul>
<li>OLS is relative easy to use.</li>
<li>The goal of minimizing \(e^2\) is appropriate from theory.</li>
<li>OLS estimates have a number of useful characteristics.</li>
</ul>
<h2 id="How-OLS-works"><a href="#How-OLS-works" class="headerlink" title="How OLS works"></a>How OLS works</h2><p>In a Single-Independent-Variable regression model:</p>
<p>$$ Y_i = \beta_0 + \beta_1 X_i + \varepsilon_i $$</p>
<p>We know that:</p>
<p>$$ \sum e^2 = \sum (Y_i - \hat{\beta_0} -\hat{\beta_1}X_i)^2 $$</p>
<p>From an equation with just one independent variable, these coefficients are:</p>
<p>$$ \hat{\beta_1} = \frac{\sum_{i=1}^N[(X_i - X)(Y_i - Y)]}{\sum_{i=1}{N}(X_i - X)^2} $$</p>
<p>$$ \hat{\beta_0} = Y - \hat{\beta_1}X $$</p>
<p>In a mutivariate regression model, we also get the estimated coefficients expression, but it’s quite complicated, we prefer a computer’s calculation.</p>
<p>But we should learn that the key is:</p>
<blockquote><p>“Specifically, a multivariate coefficient indicates the change in the dependent variable associate with a one-unit increase in the independent variable in question <code>holding constant the other independent variables in the equation</code>.”</p>
<footer><strong>A.H.Studenmund</strong><cite>-- Using Econometrics: A Pratical Guide</cite></footer></blockquote>
<h2 id="Sum-of-Squares"><a href="#Sum-of-Squares" class="headerlink" title="Sum of Squares"></a>Sum of Squares</h2><p>To get a measure of the amount of variation to be explained by the regression, we use the squared variations, which called the total sum of squares.</p>
<p>The Total Sum of Squares:</p>
<p>$$ TSS = \sum_{i=1}^N(Y_i-Y)^2 $$</p>
<p>The total sum of squares has two components:</p>
<ul>
<li>variation which can be explained by the regression – the explained sum of squares</li>
<li>variation which can’t be explained by the regression – the residual sum of squares</li>
</ul>
<p>The explained sum of squares:</p>
<p>$$ ESS = \sum_{i=1}^N(\hat{Y_i}-Y)^2 $$</p>
<p>The residual sum of squares:</p>
<p>$$ RSS = \sum_{i=1}^N(Y_i-\hat{Y_i}) $$</p>
<p>The decomposition of variance:</p>
<p>$$ TSS = ESS+ RSS $$</p>
<h2 id="Describing-the-fit-of-a-estimated-model"><a href="#Describing-the-fit-of-a-estimated-model" class="headerlink" title="Describing the fit of a estimated model"></a>Describing the fit of a estimated model</h2><p>The simplest commonly used measure of fit is the coefficient of determination, \(R^2\), which equals to:</p>
<p>$$ R^2 = \frac{ESS}{TSS} = 1 - \frac{RSS}{TSS}, and 0&lt;=R^1&lt;=1 $$</p>
<p>The higher \(R^2\) is, the closer the estimated regression equation fits it sample data.</p>
<blockquote><p>“Since OLS selects the parameter estimates that minimize RSS, OLS provides the largest possible \(R^2\), given a linear model.”</p>
<footer><strong>A.H.Studenmund</strong><cite>-- Using Econometrics: A Pratical Guide</cite></footer></blockquote>
<h2 id="Adjust-R-2"><a href="#Adjust-R-2" class="headerlink" title="Adjust \(R^2\)"></a>Adjust \(R^2\)</h2><p>If we add another independent variable to a particular equation, we will find the equation with the greater number of independent variables will always have a better (or equal) fit, and \(R^2\) won’t decrease.</p>
<p>In essence, \(R^2\) </p>
</div></article></div></section><footer><div class="paginator"><a href="/2016/zh-cn/Javascript正则表达式快速入门/" class="next">NEXT</a></div><div class="copyright"><p>© 2016 <a href="http://lumpychen.github.io">Lumpy</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script async src="//cdn.bootcss.com/mathjax/2.6.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><script>(function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;e=o.createElement(i);r=o.getElementsByTagName(i)[0];e.src='//www.google-analytics.com/analytics.js';r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));ga('create',"UA-65933410-1",'auto');ga('send','pageview');</script></body></html>